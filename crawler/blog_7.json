{
  "title": "The Journey from Jupyter to Programmer: A Quick-Start Guide",
  "author": "Lucy Dickinson",
  "date": "2025-06-04T18:22:34-05:00",
  "content": "The world’s leading publication for data science, AI, and ML professionals.\n\n\t\t\tExplore the real benefits of ditching the notebook\t\t\nMost Data Scientists, myself included, start their coding journey using a Jupyter Notebook. These files have the extension .ipynb, which stands for Interactive Python Notebook. As the extension name suggests, it has an intuitive and interactive user interface. The notebook is broken down into ‘cells’ or small blocks of separated code or markdown (text) language. Outputs are displayed underneath each cell once the code within that cell has been executed. This promotes a flexible and interactive environment for coders to build their coding skills and start working on data science projects.\nA typical example of a Jupyter Notebook is below:\nThis all sounds great. And don’t get me wrong, for use cases such as conducting solo research or exploratory data analysis (EDA), Jupyter Notebooks are great. The issues arise when you ask the following questions:\nPretty soon, the limitations of exclusively using Jupyter Notebooks within a commercial context will start to cause problems. It’s simply not designed for these purposes. The general solution is to organise code in a modular fashion.\nBy the end of this article, you should have a clear understanding of how to structure a small data science project as a Python program and appreciate the advantages of transitioning to a programming approach. You can check out an example template to supplement this article in my github here.\nThe contents of this article are based on my experience of migrating away from solely using Jupyter Notebooks to write code. Do notebooks still have a purpose? Yes. Are there alternative ways to organise and execute code beyond the methods I discuss in this article? Yes.\nI wanted to share this information to help anyone wanting to make the move away from notebooks and towards writing scripts and programs. If I’ve missed any features of Jupyter Notebooks that mitigate the limitations I’ve mentioned, please drop a comment!\nLet’s get back to it.\nFor the purpose of this article, I’ll be focusing on the Python programming language as this is the language I use for data science projects. Structuring code as a Python program unlocks a range of functionalities that are difficult to achieve when working exclusively within a Jupyter Notebook. These benefits include collaboration, versatility and portability – you’re simply able to do more with your code. I’ll explain these benefits further down – stay with me a little longer! \nPython programs are typically organised into modules and packages. A module is a python script (files with a .py extension) that contains python code which can be imported into other files. A package is a directory that contains python modules. I’ll discuss the purpose of the file __init__.py later in the article.\nAnytime you import a python library into your code, such as built-in libraries like os or third-party libraries like pandas , you are interacting with a python program that’s been organised into a package and modules.\nFor example, let’s say you want to use the randint function from numpy. This function allows you to generate a random integer based on specified parameters. You might write:\nLet’s annotate that import statement to show what you’re actually importing.\nIn this instance, numpy is a package; random is a module and randint is a function.\nSo, it turns out you probably interact with python programs on a regular basis. This poses the question, what does the journey look like towards becoming a python programmer?\nThe trick to building a functional python program is all in the file structure and organisation. It sounds boring but it plays a super important part in setting yourself up for success!\nLet me use an analogy to explain: every house has a drawer that has just about everything in it; tools, elastic bands, medicine, your hopes and dreams, the lot. There’s no rhyme or reason, it’s a dumping ground of just about everything. Think of this as a Jupyter Notebook. This one file typically contains all stages of a project, from importing data, exploring what the data looks like, visualising trends, extracting features, training a model etc. For a project that’s destined to be deployed on a production system or co-developed with colleagues, it’s going to cause chaos. What’s needed is some organisation, to put all the tools in one compartment, the medicine in another and so on.\nA great way to do that with code is to use a project template. One that I use frequently is the Cookie Cutter Data Science template. You can create a whole directory for your project with all the relevant files needed to do just about anything in a few simple operations in a terminal window – see the link above for information on how to install and run Cookie Cutter. \nBelow are some of the key features of the project template:\nTop tip. Make sure to keep Cookie Cutter up to date. With every release, new features are added according to the ever-evolving data science universe. I’ve learnt quite a few things from exploring a new file or feature in the template!\nAlternatively, you can use other templates to build your project such as that provided by Poetry. Poetry is a package manager which you can use to generate a project template that’s more lightweight than Cookie Cutter.\nThe best way to interact with your project is through an IDE (Integrated Development Environment). This software, such as Visual Studio Code (VS Code) or PyCharm, encompass a variety of features and processes that enable you to code, test, debug and package your work efficiently. My personal preference is VS Code!\nNow that we have a development environment and a nicely structured project template, how exactly do you write code in a python script if you’ve only ever coded in a Jupyter Notebook? To answer that question, let’s first consider a few industry-standard coding best practices.\nFor a deeper dive into coding best practice, this article is a fantastic overview of principles to adhere to as a Data Scientist, be sure to check it out!\nWith those best practices in mind, let’s go back to the question: how do you write code in a python script?\nFirst, separate the different stages of your notebook or project into different python files. And make sure to name them according to the task. For example, you might have the following scripts in a typical machine learning package: data.py, preprocess.py, features.py, train.py, predict.py, evaluate.py etc. Depending on your project structure, these would sit within the package or src directory.\nWithin each script, code should be organised or ‘encapsulated’ into a classes and/or functions. A function is a reusable block of code that performs a single, well-defined task. A class is a blueprint for creating an object, with its own set of attributes (variables) and methods (functions). Encapsulating code in this manner permits reusability and avoids duplication, thus keeping code concise.\nA script might only need one function if the task is simple. For example, a data loading module (e.g. data.py) may only contain a single function ‘load_data’ which loads data from a csv file into a pandas DataFrame. Other scripts, such as a data processing module (e.g. preprocess.py) will inherently involve more tasks and hence requires more functions or a class to encapsulate these tasks.\nTop tip. Transitioning from Jupyter Notebooks to scripts may take some time and everyone’s personal journey will look different. Some Data Scientists I know write code as python scripts straight away and don’t touch a notebook. Personally, I use a notebook for EDA, I then encapsulate the code into functions or classes before porting to a script. Do whatever feels right for you.\nThere are a few tools that can help with the transition. 1) In VS Code, you can select one or more lines, right click and select Run Python > Run Selection/Line in Python Terminal. This is similar to running a cell in Jupyter Notebook. 2) You can convert a notebook to a python script by clicking File > Download as > Python (.py). I wouldn’t recommend that approach with large notebooks for fear of creating monster scripts, but the option is there!\nAt this point, we’ve established that code should be encapsulated into functions and stored within clearly named scripts. The next logical question is, how can you tie all these scripts together so code gets executed in the right order?\nThe answer is to import these scripts into a single-entry point and execute the code in one place. Within the context of developing a simple project, this entry point is typically a script named main.py (but can be called anything). At the top of main.py, just as you would import necessary built-in packages or third-party packages from PyPI, you’ll import your own modules or specific classes/functions from modules. Any classes or functions defined in these modules will be available to use by the script they’ve been imported into.\nTo do this, the package directory within your project needs to contain a __init__.py file, which is typically left blank for simple projects. This file tells the python interpreter to treat the directory as a package, meaning that any files with a .py extension get treated as modules and can therefore be imported into other files.\nThe structure of main.py is project dependent, but it will generally be dictated by the necessary order of code execution. For a typical machine learning project, you would first need to use the load_data function from the module data.py. You then might instantiate the preprocessor class that is imported from the module preprocess.py and apply a variety of class methods to the preprocessor object. You would then move onto feature engineering and so on until you have the whole workflow written out. This workflow would typically be contained or referenced within a conditional statement at the bottom of main.py.\nWait….. who mentioned anything about a conditional statement? The conditional statement is as follows:\n__name__ is a special python variable that can have two different values depending on how the script is run:\nSome more information on this can be found here.\nGiven this process, you’ll need to reference the master function within the if '__name__=='__main__': conditional statement so that it is executed when main.py is run. Alternatively, you can place the code underneath if '__name__=='__main__': to achieve the same outcome. \nmain.py (or any python script) can be executed in terminal using the following syntax:\nUpon running main.py, code will be executed from all the imported modules in the specified order. This is the same as clicking the ‘run all’ button on a Jupyter Notebook where each cell is executed in sequential order. The difference now is that the code is organised into individual scripts in a logical manner and encapsulated within classes and functions. \nYou can also add CLI (command-line interface) arguments to your code using tools such as argparse and typer, allowing you to toggle specific variables when running main.py in the terminal. This provides a great deal of flexibility during code execution.\nSo we’ve now reached the best part. The pièce de résistance. The real reasons why, beyond having fantastically organised and readable code, you should go to the effort of programming.\nLet’s walk through some of the key benefits of moving beyond Jupyter Notebooks and transitioning to writing Python scripts instead.\nFun fact. It’s generally discouraged to commit Jupyter Notebooks to version control systems as it is difficult to track changes!\nNote. If you use a YAML file (.yml) for configuration, this requires the python package yaml. Make sure to install the pyyaml package (not ‘yaml’) using pip install pyyaml. Forgetting this can lead to “package not found” errors—I’ve made this mistake, maybe more than once..\nAs I eluded at the beginning of this article, Jupyter Notebooks still have their place in data science projects. Their easy-to-use interface makes them great for exploratory and interactive tasks. Two key use cases are listed below:\nThanks for sticking with me to the very end! I hope this discussion has been insightful and has shed some light on how and why to start programming. As with most things in Data Science, there isn’t a single ‘correct’ way to solve a problem, but a considered multi-faceted approach depending on the task at hand.\nShout out to my colleague and fellow data scientist Hannah Alexander for reviewing this article 🙂\nThanks for reading!\nWritten By\nTopics:\nShare this article:\nDerivation and practical examples of this powerful concept \nWith demos, our new solution, and a video \nPart three of a comprehensive, practical guide to CLV techniques and real-world use-cases \nHow to build penalized quantile regression models (with code!) \nThis is a bit different from what the books say. \nFabric Madness part 3 \nHow to be more pragmatic as a Data Scientist, and why it matters for your… \nYour home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nExploring the Proportional Odds Model for Ordinal Logistic Regression | Towards Data Science\nApplications of Density Estimation to Legal Theory | Towards Data Science\nMastering SQL Window Functions | Towards Data Science\nExploratory Data Analysis: Gamma Spectroscopy in Python | Towards Data Science",
  "url": "https://towardsdatascience.com/the-journey-from-jupyter-to-programmer-a-quick-start-guide/"
}