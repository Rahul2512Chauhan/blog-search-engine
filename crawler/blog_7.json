{
  "title": "Data Drift Is Not the Actual Problem: Your Monitoring Strategy Is",
  "author": "Mahe Jabeen Abdul",
  "date": "2025-06-03T18:24:53-05:00",
  "content": "The world’s leading publication for data science, AI, and ML professionals.\n\n\t\t\tMonitoring is easy; what to monitor is not. In the field of machine learning, data drift is\njust noise until you know what it means.\t\t\nMachine learning is an approach to accuracy that devours data, learns patterns, and predicts. However, with the best models, even those predictions could crumble in the real world without a sound. Companies using machine learning systems tend to ask the same question: What went wrong?\nThe standard thumb rule answer is “data drift”. If the properties of your customers, transactions or images change because of the distribution of the incoming data, the model’s understanding of the world becomes outdated. Data drift, however, is not a real problem but a symptom. I think the real concern is that most organizations monitor data without understanding it.\nIn my experience, most machine learning teams are taught to look for data drift only after the performance of the model deteriorates. Statistical drift detection is the industry’s automatic reaction to instability. However, even though statistical drift can demonstrate that data has changed, it rarely explains what the change means or if it is important.\nOne of the examples I tend to give is Google Cloud’s Vertex AI, which offers an out-of-the-box drift detection system. It can track feature distributions, see them go out of normal distributions, and even automate retraining when drift exceeds a predefined threshold. This is ideal if you are only worried about statistical alignment. However, in most businesses, that is not sufficient.\nAn e-commerce firm that I was involved in incorporated a product recommendation model. During the holiday season, customers tend to shift from everyday needs to the purchase of gifts. What I saw was that the input data of the model altered product categories, price ranges, and frequency of purchases which all drifted. A conventional drift detection system may cause alerts but it is normal behavior and not a problem. Viewing it as a problem may lead to the unnecessary retraining or even misleading changes in the model.\nI have collaborated with various organizations that build their monitoring pipelines on statistical thresholds. They use measures such as the Population Stability Index (PSI), Kullback-Leibler Divergence (KL Divergence), or Chi-Square tests to detect changes in data distributions. These are accurate but naive metrics; they don’t understand context.\nTake AWS SageMaker’s Model Monitor as a real-world example. It has tools that automatically notice changes in input features by comparing live data with a reference set. You may set alerts in CloudWatch to monitor when a feature’s PSI reaches a set limit. Still, it’s a helpful start, but it doesn’t say whether the changes are important.\nImagine that you are using a loan approval model in your business. If the marketing team introduces a promotion for bigger loans at better rates, Model Monitor will notice that the loan amount feature is not as accurate. Still, this is done on purpose, because retraining could override fundamental changes in the business. The key problem is that, without knowledge of the business layer, statistical monitoring can result in wrong actions.\nIf drift detection alone doesn’t solve the real problem, then what does? A good monitoring system should go beyond statistics and be a reflection of the business outcomes that the model should deliver. This requires a three-layered approach:\nStatistical monitoring should be your first line of defence. Metrics like PSI, KL Divergence, or Chi-Square can be used to identify the fast change in the distribution of features. However, they must be viewed as signals and not alarms.\nMy marketing team launched a series of promotions for new-users of a subscription-based streaming service. During the campaign, the distributions of features for “user age”, “signup source”, and “device type” all underwent substantial drifts. However, rather than provoking retraining, the monitoring dashboard placed these shifts next to the metrics of the campaign performance, which showed that they were expected and time-limited.\nContextual monitoring aligns technical signals with business meaning. It answers a deeper question than “Has something drifted?” It asks, “Does the drift affect what we care about?”\nGoogle Cloud’s Vertex AI offers this bridge. Alongside basic drift monitoring, it allows users to configure slicing and segmenting predictions by user demographics or business dimensions. By monitoring model performance across slices (e.g., conversion rate by customer tier or product category), teams can see not just that drift occurred, but where and how it impacted business outcomes.\nIn an e-commerce application, for instance, a model predicting customer churn may see a spike in drift for “engagement frequency.” But if that spike correlates with stable retention across high-value customers, there’s no immediate need to retrain. Contextual monitoring encourages a slower, more deliberate interpretation of drift tuned to business priorities.\nApart from inputs, your model’s output should be monitored for abnormalities. This is to track the model’s predictions and the results that they create. For instance, in a financial institution where a credit risk model is being implemented, monitoring should not only detect a change in the users’ income or loan amount features. It should also track the approval rate, default rate, and profitability of loans issued by the model over time.\nIf the default rates for approved loans skyrocket in a certain region, that is a big issue even if the model’s feature distribution has not drifted.\nA sound monitoring system isn’t a visual dashboard or a checklist of drift metrics. It’s an embedded system within the ML architecture capable of distinguishing between harmless change and operational threat. It must help teams interpret change through multiple layers of perspective: mathematical, business, and behavioral. Resilience here means more than uptime; it means knowing what changed, why, and whether it matters.\nAt this layer, the goal is to detect signal variation as early as possible but to treat it as a prompt for inspection, not immediate action. Metrics like Population Stability Index (PSI), KL Divergence, and Chi-Square tests are widely used here. They flag when a feature’s distribution diverges significantly from its training baseline. But what’s often missed is how these metrics are applied and where they break.\nIn a scalable production setup, statistical drift is monitored on sliding windows, for example, a 7-day rolling baseline against the last 24 hours, rather than against a static training snapshot. This prevents alert fatigue caused by models reacting to long-passed seasonal or cohort-specific patterns. Features should also be grouped by stability class: for example, a model’s “age” feature will drift slowly, while “referral source” might swing daily. By tagging features accordingly, teams can tune drift thresholds per class instead of globally, a subtle change that significantly reduces false positives.\nThe most effective deployments I’ve worked on go further: They log not only the PSI values but also the underlying percentiles explaining where the drift is happening. This enables faster debugging and helps determine whether the divergence affects a sensitive user group or just outliers.\nWhere the statistical layer asks “what changed?”, the contextual layer asks “why does it matter?” This layer doesn’t look at drift in isolation. Instead, it cross-references changes in input distributions with fluctuations in business KPIs.\nFor example, in an e-commerce recommendation system I helped scale, a model showed drift in “user session duration” during the weekend. Statistically, it was significant. However, when compared to conversion rates and cart values, the drift was harmless; it reflected casual weekend browsing behavior, not disengagement. Contextual monitoring resolved this by linking each key feature to the business metric it most influenced (e.g., session duration → conversion). Drift alerts were only considered critical if both metrics deviated together.\nThis layer often also involves segment-level slicing, which looks at drift not in global aggregates but within high-value segments. When we applied this to a subscription business, we found that drift in signup device type had no impact overall, but among churn-prone cohorts, it strongly correlated with drop-offs. That difference wasn’t visible in the raw PSI, only in a slice-aware context model.\nEven when the input data seems unchanged, the model’s predictions can begin to diverge from real-world outcomes. That’s where the behavioral layer comes in. This layer tracks not only what the model outputs, but also how those outputs perform.\nIt’s the most neglected but most critical part of a resilient pipeline. I’ve seen a case where a fraud detection model passed every offline metric and feature distribution check, but live fraud loss began to rise. Upon deeper investigation, adversarial patterns had shifted user behavior just enough to confuse the model, and none of the earlier layers picked it up.\nWhat worked was tracking the model’s outcome metrics, chargeback rate, transaction velocity, approval rate, and comparing them against pre-established behavioral baselines. In another deployment, we monitored a churn model’s predictions not only against future user behavior but also against marketing campaign lift. When predicted churners received offers and still didn’t convert, we flagged the behavior as “prediction mismatch,” which told us the model wasn’t aligned with current user psychology, a kind of silent drift most systems miss.\nThe behavioral layer is where models are judged not on how they look, but on how they behave under stress.\nNot all drift is problematic, and not all alerts are actionable. Sophisticated monitoring pipelines embed conditional alerting logic that decides when drift crosses the threshold into risk.\nIn one pricing model used at a regional retail chain, we found that category-level price drift was entirely expected due to supplier promotions. Still, user segment drift (especially for high-spend repeat customers) signaled profit instability. So the alerting system was configured to trigger only when drift coincided with a degradation in conversion margin or ROI.\nConditional alerting systems need to be aware of feature sensitivity, business impact thresholds, and acceptable volatility ranges, often represented as moving averages. Alerts that aren’t context-sensitive are ignored; those that are over-tuned miss real issues. The art is in encoding business intuition into monitoring logic, not just thresholds.\nJust like your model code, your monitoring logic becomes stale over time. What was once a valid drift alert may later become noise, especially after new users, regions, or pricing plans are introduced. That’s why mature teams conduct scheduled reviews not just of model accuracy, but of the monitoring system itself.\nIn a digital payment platform I worked with, we saw a spike in alerts for a feature tracking transaction time. It turned out the spike correlated with a new user base in a time zone we hadn’t modeled for. The model and data were fine, but the monitoring config was not. The solution wasn’t retraining; it was to realign our contextual monitoring logic to revenue-per-user group, not global metrics.\nValidation means asking questions like: Are your alerting thresholds still tied to business risk? Are your features still semantically valid? Have any pipelines been updated in ways that silently affect drift behavior?\nMonitoring logic, like data pipelines, must be treated as living software, subject to testing and refinement.\nOne of the biggest mistakes in machine learning ops is to treat monitoring thresholds and logic as an afterthought. In reality, these configurations are just as mission-critical as the model weights or the preprocessing code.\nIn robust systems, monitoring logic is stored as version-controlled code: YAML or JSON configs that define thresholds, slicing dimensions, KPI mappings, and alert channels. These are committed alongside the model version, reviewed in pull requests, and deployed through CI/CD pipelines. When drift alerts fire, the monitoring logic that triggered them is visible and can be audited, traced, or rolled back.\nThis discipline prevented a significant outage in a customer segmentation system we managed. A well-meaning config change to drift thresholds had silently increased sensitivity, leading to repeated retraining triggers. Because the config was versioned and reviewed, we were able to identify the change, understand its intent, and revert it  all in under an hour.\nTreat monitoring logic as part of your infrastructure contract. If it’s not reproducible, it’s not reliable.\nI believe data drift is not an issue. It’s a signal. But it is too often misinterpreted, leading to unjustified panic or, even worse, a false sense of security. Mere monitoring is more than statistical thresholds. It is knowing the impact of the change in data on your business.\nThe future of monitoring is context-specific. It needs systems that can separate noise from signal, detect drift, and appreciate its significance. If your model’s monitoring system cannot answer the question “Does this drift matter?”. It is not monitoring.\nWritten By\nTopics:\nShare this article:\nStep-by-step code guide to building a Convolutional Neural Network \nA beginner’s guide to forecast reconciliation \nHere’s how to use Autoencoders to detect signals with anomalies in a few lines of… \nFeature engineering, structuring unstructured data, and lead scoring \nAn illustrated guide on essential machine learning concepts \nDerivation and practical examples of this powerful concept \nColumns on TDS are carefully curated collections of posts on a particular idea or category… \nYour home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nAutomate Models Training: An MLOps Pipeline with Tekton and Buildpacks | Towards Data Science\n10,000x Faster Bayesian Inference: Multi-GPU SVI vs. Traditional MCMC | Towards Data Science\nLanding your First Machine Learning Job: Startup vs Big Tech vs Academia | Towards Data Science\nYour DNA Is a Machine Learning Model: It’s Already Out There | Towards Data Science",
  "url": "https://towardsdatascience.com/data-drift-is-not-the-actual-problem-your-monitoring-strategy-is/"
}