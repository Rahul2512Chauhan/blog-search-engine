{
  "title": "A Bird’s-Eye View of Linear Algebra: Measure of a Map — Determinants",
  "author": "Rohit Pandey",
  "date": "2025-06-10T00:00:27-05:00",
  "content": "The world’s leading publication for data science, AI, and ML professionals.\n\n\t\t\tWe roll up our sleeves and start to deal with matrices\t\t\nThis is the second chapter in the in-progress book on linear algebra. The table of contents so far:\nStay tuned for future chapters.\n\nLinear algebra is the tool of many dimensions. No matter what you might be doing, as soon as you scale to 𝑛 dimensions, linear algebra comes into the picture.\nIn the previous chapter, we described abstract linear maps. In this one, we roll up our sleeves and start to deal with matrices. Practical considerations like numerical stability, efficient algorithms, etc. will now start to be explored.\nNote: all images in this article, unless otherwise stated are by the author.\nDeterminants are one of the most ancient concepts in linear algebra. The roots of the subject lay in solving systems of linear equations. And determinants would “determine” if there even was a solution worth looking for. But in most of the cases, where the system does have a solution, it provides further useful information. In the modern framework of linear maps, determinants provide a single quantification of linear maps.\nWe discussed in the previous chapter the concept of vector spaces (basically n-dimensional collections of numbers — and more generally collections of fields) and linear maps that operate on two of those vector spaces, taking objects in one to the other.\nAs an example of these kinds of maps, one vector space could be the surface of the planet you’re sitting on and the other could be the surface of the table you might be sitting at. Literal maps of the world are also maps in this sense since they “map” every point on the surface of the Earth to a point on a paper or surface of a table, although they aren’t linear maps since they don’t preserve relative areas (Greenland appears much larger than it is for example in some of the projections).\nOnce we pick a basis for the vector space (a collection of n “independent” vectors in the space; there could be infinite choices in general), all linear maps on that vector space get unique matrices assigned to them.\nFor the time being, let’s restrict our attention to maps that take vectors from an 𝑛-dimensional space back to the 𝑛-dimensional space (we’ll generalize later). The matrices corresponding to these linear maps are 𝑛×𝑛 (see section III of chapter 1). It might be useful to “quantify” such a linear map, express its effect on the vector space, ℝⁿ in a single number. The kind of map we’re dealing with, effectively takes vectors from ℝⁿ and “distorts” them into some other vectors in the same space. Both the original vector 𝑣 and the vector 𝑢 that the map converted it into have some lengths (say |𝑣| and |𝑢|). We can think about how much the length of the vector is changed by the map, |𝑢|∕|𝑣|. Maybe that can quantify the impact of the map? How much it “stretches” vectors?\nThis approach has a fatal flaw. The ratio depends not just on the linear map, but also on the vector 𝑣 it acts on. It is therefore not strictly a property of the linear map itself.\nWhat if we take two vectors instead now, 𝑣₁ and 𝑣₂ which are converted by the linear map into the vectors 𝑢₁ and 𝑢₂. Just as the measure of the single vector, 𝑣 was its length, the measure of two vectors is the area of the parallelogram contained between them.\nJust as we considered the amount by which the length of 𝑣 changed, we can now talk in terms of the amount by which the area between 𝑣₁ and 𝑣₂ changes once they pass through the linear map and become 𝑢₁, 𝑢₂. And alas, this again depends not just on the linear map, but also the vectors chosen.\nNext, we can go to three vectors and consider the change in volume of the parallelepiped between them and run into the same problem of the initial vectors having a say.\nBut now consider an n-dimensional region in the original vector space. This region will have some “n-dimensional measure”. To understand this, a two dimensional measure is an area (measured in square kilometers). A three dimensional measure is the volume used for measuring water (in liters). A four dimensional measure has no counterpart in the physical world we’re used to, but is just as mathematically sound, a measure of the amount of four dimensional space enclosed within a parallelepiped formed of four 4- d vectors and so on.\nThe 𝑛 original vectors (𝑣₁, 𝑣₂, …, 𝑣ₙ) form a parallelepiped which is transformed by the linear map into 𝑛 new vectors, 𝑢₁, 𝑢₂, …, 𝑢ₙ which form their own parallelepiped. We can then ask about the 𝑛-dimensional measure of the new region in relation to the original one. And this ratio, it turns out, is indeed a function only of the linear map. Regardless of what the original region looked like, where it was placed and so on, the ratio of its measure once the linear map acted on it to its measure before will be the same — a function purely of the linear map. This ratio of 𝑛-dimensional measures (after to before) then is what we’ve been looking for: an exclusive property of the linear map that quantifies its effect in one number.\nThis ratio by which the measure of any 𝑛-dimensional patch of space is changed by the linear map is a good way to quantify the effect it has on the space it acts on. It is called the determinant of the linear map (the reason for that name will become apparent in section V).\nFor now, we simply stated the fact that the amount by which a linear map from ℝⁿ to ℝⁿ “stretches” any patch of 𝑛-dimensional space depends only on the map without offering a proof since the purpose here was motivation. We’ll cover a proof later (section VI), once we arm ourselves with some weapons.\nNow, how do we find this determinant given a linear map from the vector space ℝⁿ back to ℝⁿ? We can take any 𝑛 vectors, find the measure of the parallelepiped between them and the measure of the new parallelepiped once the linear map has acted on all of them. Finally, divide the latter by the former.\nWe need to make these steps more concrete. First, let’s start playing around in this ℝⁿ vector space.\nThe ℝⁿ vector space is just a collection of 𝑛 real numbers. The simplest vector is just 𝑛 zeros — [0, 0, …, 0]. This is the zero vector. If we multiply a scalar with it, we just get the zero vector back. Not interesting. For the next simplest vector, we can replace the first 0 with a 1. This leads to the vector: 𝑒₁ = [1, 0, 0, …, 0]. Now, multiplying by a scalar, 𝑐 gives us a different vector.\n𝑐.[1,0,0,..,0]=[𝑐,0,0,…,0]\nWe can “span” an infinite number of vectors with 𝑒₁ depending on the scalar 𝑐 we choose.\nIf 𝑒₁ is the vector with just the first element being 1 and the rest being 0, then what is 𝑒₂? The second element being 1 and the rest being 0 seems like a logical choice.\n𝑒2=[0,1,0,0,…0]\nTaking this to its logical conclusion, we get a collection of n vectors:\nThese vectors form a basis of the vector space that is ℝⁿ. What does this mean? Any vector 𝑣 in ℝⁿ can be expressed as a linear combination of these 𝑛 vectors. Which means that for some scalars 𝑐₁, 𝑐₂, …, 𝑐ₙ:\n𝑣=𝑐1.𝑒1+𝑐2.𝑒2+⋯+𝑐𝑛.𝑒𝑛\nAll vectors, 𝑣 are “spanned” by the set of vectors 𝑒₁, 𝑒₂, …, 𝑒ₙ.\nThis particular collection of vectors isn’t the only basis. Any set of 𝑛 vectors works. The only caveat is that none of the 𝑛 vectors should be “spanned” by the rest. In other words, all the 𝑛 vectors should be linearly independent. If we choose 𝑛 random numbers from most continuous distributions and repeat the process 𝑛 times to create the 𝑛 vectors, you will get a set of linearly independent vectors with 100% probability (“almost surely” in probability terms). It’s just very, very unlikely that a random vector happens to be “spanned” by some other 𝑘 < 𝑛 random vectors.\nGoing back to our recipe at the beginning of this section to find the determinant of a linear map, we now have a basis to express our vectors in. Fixing the basis also means our linear map can be expressed as a matrix (see section III of chapter 1). Since this linear map is taking vectors from ℝⁿ back to ℝⁿ, the corresponding matrix is 𝑛 × 𝑛.\nNext, we needed 𝑛 vectors to form our parallelepiped. Why not take the 𝑒₁, 𝑒₂, …, 𝑒ₙ standard basis we defined before? The measure of the patch of space contained between these vectors happens to be 1, by very definition. The picture below for ℝ³ will hopefully make this clear.\nIf we collect these vectors from the standard basis into a matrix (rows or columns), we get the identity matrix (1’s on the main diagonal, 0’s everywhere else):\nWhen we said we could apply our linear transform to any n-dimensional patch of space, we might as well apply it to this “standard” patch.\nBut, it’s easy to show that multiplying any matrix with the identity matrix results in the same matrix. So, the resulting vectors after the linear map is applied are the columns of the matrix representing the linear map itself. So, the amount by which the linear map changed the volume of the “standard patch” is the same as the n-dimensional measure of the parallelepiped between the column vectors of the matrix representing the map itself.\nTo recap, we started by motivating the determinant as the ratio by which a linear map changes the measure of an n-dimensional patch of space. And now, we showed that this ratio itself is an n-dimensional measure. In particular, the measure contained between the column vectors of any matrix representing the linear map.\nWe described in the previous section how a determinant of a linear map should simply be the measure contained between the vectors of any of its matrix representations. In this section, we use two dimensional space (where measures are areas) to motivate some fundamental properties a determinant must have.\nThe first property is multi-linearity. A determinant is a function that takes a bunch of vectors (collected in a matrix) and maps them to a single scalar. Since we’re restricting to two-dimensional space, we’ll consider two vectors, both two dimensional. Our determinant (since we’ve motivated it to be the area of the parallelogram between the vectors) can be expressed as:\n𝑑𝑒𝑡=𝐴(𝑣1,𝑣2)\nHow should this function behave if we add a vector to one of the two vectors? The multi-linearity property requires:\n𝐴(𝑣1+𝑣3,𝑣2)=𝐴(𝑣1,𝑣2)+𝐴(𝑣3,𝑣2)(1)This is apparent from the moving picture below (note the new area getting added).\nAnd this visualization can also be used to see (by scaling one of the vectors instead of adding another vector to it):𝐴(𝑐.𝑣1,𝑣2)=𝑐.𝐴(𝑣1,𝑣2)(2)This second property has an important implication. What if we plug a negative c into the equation?\nThe area, 𝐴(𝑣₁, 𝑣₂) should then be the opposite sign to 𝐴(𝑐·𝑣₁, 𝑣₂).\nWhich means we need to introduce the notion of negative area and a negative determinant.\nThis makes a lot of sense if we’re okay with the concept of negative lengths. If lengths — measures in 1-D space — can be positive or negative, then it stands to reason that areas — measures in 2-D space — should also be allowed to be negative. And so, measures in space of any dimensionality should as well.\nTogether, equations (1) and (2) are the multi-linearity property.\nAnother important property that has to do with the sign of the determinant is the alternating property. It requires:\n𝐴(𝑣1,𝑣2)=−𝐴(𝑣2,𝑣1)\nSwapping the order of two vectors negates the sign of the determinant (or measure between them). If you learned about the cross product of 3-D vectors, this property will be very natural. To motivate it, let’s think first of the one-dimensional distance between two position vectors, 𝑑(𝑣₁, 𝑣₂). It’s clear that 𝑑(𝑣₁, 𝑣₂) = −𝑑(𝑣₂, 𝑣₁) since when we go from 𝑣₂ to 𝑣₁, we’re traveling in the opposite direction to when we go from 𝑣₁ to 𝑣₂. Similarly, if the area spanned between vectors 𝑣₁ and 𝑣₂ is positive, then that between 𝑣₂ and 𝑣₁ must be negative. This property holds in 𝑛-dimensional space as well. If in 𝐴(𝑣₁, 𝑣₂, …, 𝑣ₙ) we swap two of the vectors, it causes the sign to switch.\nThe alternating property also implies that if one of the vectors is simply a scalar multiple of the other, the determinant must be 0. This is because swapping the two vectors should negate the determinant:\n𝐴(𝑣1,𝑣1)=−𝐴(𝑣1,𝑣1) =>2𝐴(𝑣1,𝑣1)=0 =>𝐴(𝑣1,𝑣1)=0\nWe also have by multi-linearity (equation 2):𝐴(𝑣1,𝑐.𝑣1)=𝑐𝐴(𝑣1,𝑣1)=0This makes sense geometrically since if two vectors are parallel to each other, the area between them is 0.\nThe video [6] covers the geometric motivation of these properties with really good visualizations and video [4] visualizes the alternating property quite well.\nIn this section, we move away from geometric intuition and approach the topic of determinants from an alternate route — that of cold, algebraic calculations.\nSee, the multi-linearity and alternating properties which we motivated in the last section with geometry are (remarkably) enough to give us a very specific algebraic formula for the determinant, called the Leibniz formula.\nThat formula helps us see properties of the determinant that would be really, really hard to observe from the geometric approach or with other algebraic formulas.\nThe Leibniz formula can then be reduced to the Laplace expansion, involving going along a row or column and calculating cofactors — which many people see in high school.\nLet’s derive the Leibniz formula. We need a function that takes the 𝑛 column vectors, 𝛼₁, 𝛼₂, …, 𝛼ₙ of the matrix as input and converts them into a scalar, 𝑐.\n𝑐=𝑓(𝑎1→,𝑎2→,…𝑎𝑛→)\nWe can express each column vector in terms of the standard basis of the space.\nNow, we can apply the property of multi-linearity. For now, to the first column, 𝛼₁.\nWe can do the same for the second column. Let’s take just the first term from the summation above and take a look at the resulting terms.\nNote that in the first term, we get the vector 𝑒₁ appearing twice. And by the alternating property, the function 𝑓 for that term becomes 0.\nIn order for two 𝑒₁’s to appear, the second indices of the two 𝑎’s in the product must each become 1.\nSo, once we do this for all the columns, the terms that won’t become zero by the alternating property will be the ones where the second indices of the 𝑎’s don’t have any repetition — so all distinct numbers from 1 to 𝑛. In other words, we’re looking for permutations of 1 to 𝑛 to appear in the second indices of the 𝑎’s.\nWhat about the first indices of the 𝑎’s? Those are simply the numbers 1 to 𝑛 in order since we pull out the 𝑎₁ₓ’s first, then the 𝑎₂ₓ’s, and so on. In more compact algebraic notation,\nIn the expression on the right, the areas 𝑓(𝑒_{𝑗₁}, 𝑒_{𝑗₂}, …, 𝑒_{𝑗ₙ}) can either be +1, −1, or 0 since the 𝑒ⱼ’s are all unit vectors orthogonal to each other. We already established that any term that has any repeated 𝑒ⱼ’s will become 0, leaving us with just permutations (no repetition). Among those permutations, we will sometimes get +1 and sometimes −1.\nThe concept of permutations carries with it signs. The signs of the areas are equivalent to the signs of the permutations. If we denote by 𝑆ₙ the set of all permutations of [1, 2, …, 𝑛], then we get the Leibniz formula of the determinant:\ndet([𝑎1→,𝑎2→,…𝑎𝑛→])=|𝐴|=∑𝜎∈𝑆𝑛𝑠𝑔𝑛(𝜎)∏𝑖=1𝑛𝑎𝑖,𝜎(𝑖)(3)\nThis formula is also described in detail in mathexchange post, [3]. And to make things concrete, here is some simple Python code that implements it (along with a test case).\nOne shouldn’t actually use this formula to calculate the determinant of a matrix (unless it’s just for fun or exposition). It works, but is comically inefficient given the sum over all permutations (which is 𝑛!, which is super-exponential).\nHowever, many theoretical properties of the determinant become trivial to see with the Leibniz formula when they would be very hard to decipher or prove if we started from another of its forms. For example:\nThe third fact actually leads to the most efficient algorithm for calculating a determinant that most linear algebra libraries use. A matrix can be decomposed efficiently into lower and upper triangular matrices (called the LU decomposition which we’ll cover in the next chapter). After doing this decomposition, the third fact is used to multiply the diagonals of those lower and upper matrices to get their determinants. And finally, the second fact is used to multiply those two determinants and get the determinant of the original matrix.\nA lot of people in high school or university when first exposed to the determinant, learn about the Laplace expansion, which involves expanding about a row or column, finding co-factors for each element and summing. That can be derived from the above Leibniz expansion by collecting similar terms. See this answer to the mathexchange post, [2].\nThe determinant was first discovered in the context of linear systems of equations. Say we have 𝑛 equations in 𝑛 variables (𝑥₀, 𝑥₁, …, 𝑥ₙ).\nThis system can be expressed in matrix form:\nAnd more compactly:\n𝐴.𝑥=𝑏\nAn important question is whether or not the system above has a unique solution, x. And the determinant is a function that “determines” this. There is a unique solution if and only if the determinant of A is non-zero.\nThis historically inspired approach motivates the determinant as a polynomial that arises when we try to solve a linear system of equations associated with the linear map. We will cover this in more depth in chapter 5.\nFor more on this, see the excellent answer in the mathexchange post, [8].\nWe started this chapter by motivating the determinant as the amount by which the ℝⁿ → ℝⁿ linear map changes the measure of an n-dimensional patch of space. We also said that this doesn’t work for 1, 2, … n − 1 dimensional measures. Below is a proof of this where we use some of the properties we encountered in the rest of the sections.\nDefine (𝑉, 𝑈) as 𝑛 × 𝑘 matrices, where\n𝑉=(𝑣1,𝑣2,…,𝑣𝑘)\nBy definition,\n|𝑣1,𝑣2,…,𝑣𝑘|=det(𝑉𝑡𝑉)‾‾‾‾‾‾‾‾‾√ and\n|𝑢1,𝑢2,…,𝑢𝑘|=det(𝑈𝑡𝑈)‾‾‾‾‾‾‾‾‾√=det((𝐴𝑉)𝑡(𝐴𝑉))‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√=det(𝑉𝑡𝐴𝑡𝐴𝑉)‾‾‾‾‾‾‾‾‾‾‾‾‾√ \n\nOnly when n = k is V is a square matrix, so\n|𝑣1,𝑣2,…,𝑣𝑘|=det(𝑉𝑡𝐴𝑡𝐴𝑉)‾‾‾‾‾‾‾‾‾‾‾‾‾√\n=det(𝑉𝑡)det(𝐴𝑡)det(𝐴)det(𝑉)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√=det(𝐴)det(𝑉𝑡𝑉)‾‾‾‾‾‾‾‾‾√=det(𝐴)|𝑣1,𝑣2,…,𝑣𝑘|\n[1] Mathexchange post: Determinant of a linear map doesn’t depend on the bases: https://math.stackexchange.com/questions/962382/determinant-of-linear-transformation\n[2] Mathexchange post: Determinant of a matrix Laplace expansion (high school formula) https://math.stackexchange.com/a/4225580/155881\n[3] Mathexchange post: Understanding Leibniz formula for determinants https://math.stackexchange.com/questions/319321/understanding-the-leibniz-formula-for-determinants#:~:text=The%20formula%20says%20that%20det,permutation%20get%20a%20minus%20sign.&text=where%20the%20minus%20signs%20correspond%20to%20the%20odd%20permutations%20from%20above.\n[4] Youtube video: 3B1B on determinants https://www.youtube.com/watch?v=Ip3X9LOh2dk&t=295s\n[5] Connecting Leibniz formula with geometry https://math.stackexchange.com/questions/593222/leibniz-formula-and-determinants\n[6] Youtube video: Leibniz formula is area: https://www.youtube.com/watch?v=9IswLDsEWFk\n[7] Mathexchange post: product of determinants is determinant of product https://math.stackexchange.com/questions/60284/how-to-show-that-detab-deta-detb\n[8] Historic context for motivating determinant: https://math.stackexchange.com/a/4782557/155881\nWritten By\nTopics:\nShare this article:\nOur weekly selection of must-read Editors’ Picks and original features \nA concrete case study \nHow to adjust standard errors for heteroscedasticity and why it works \nA guide to the most popular techniques when randomized A/B testing is not possible \nNature creates patterns in numbers and dictates that some are more privileged in terms of… \nVector Equations and Spans \nPreface Welcome back to the fourth edition of my ongoing series on the basics of… \nYour home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nExploring the Proportional Odds Model for Ordinal Logistic Regression | Towards Data Science\nHow to Transition From Data Analyst to Data Scientist | Towards Data Science\nExploratory Data Analysis: Gamma Spectroscopy in Python | Towards Data Science\nMastering SQL Window Functions | Towards Data Science",
  "url": "https://towardsdatascience.com/a-birds-eye-view-of-linear-algebra-measure-of-a-map-determinants/"
}