{
  "title": "Decision Trees Natively Handle Categorical Data",
  "author": "Vadim Arzamasov",
  "date": "2025-06-03T13:27:12-05:00",
  "content": "The world’s leading publication for data science, AI, and ML professionals.\n\n\t\t\tBut mean target encoding is their turbocharger\t\t\nMany claim that machine learning algorithms can’t handle categorical variables. But decision trees (DTs) can. Classification trees don’t require a numerical target either. Below is an illustration of a tree that classifies a subset of Cyrillic letters into vowels and consonants. It uses no numeric features — yet it exists.\nMany also promote mean target encoding (MTE) as a clever way to convert categorical data into numerical form — without inflating the feature space as one-hot encoding does. However, I haven’t seen any mention of this inherent connection between MTE and decision tree logic on TDS. This article addresses exactly that gap through an illustrative experiment. In particular:\nA quick note: One-hot encoding is often portrayed unfavorably by fans of mean target encoding — but it’s not as bad as they suggest. In fact, in our benchmark experiments, it often ranked first among the 32 categorical encoding methods we evaluated. [1]\nDecision tree learning is a recursive algorithm. At each recursive step, it iterates over all features, searching for the best split. So, it’s enough to examine how a single recursive iteration handles a categorical feature. If you’re unsure how this operation generalizes to the construction of the full tree, take a look here [2].\nFor a categorical feature, the algorithm evaluates all possible ways to divide the categories into two nonempty sets and selects the one that yields the highest split quality. The quality is typically measured using Gini impurity for binary classification or mean squared error for regression — both of which are better when lower. See their pseudocode below.\nLet’s say the feature has cardinality k. Each category can belong to either of the two sets, giving 2ᵏ total combinations. Excluding the two trivial cases where one of the sets is empty, we’re left with 2ᵏ−2 feasible splits. Next, note that we don’t care about the order of the sets — splits like {{A,B},{C}} and {{C},{A,B}} are equivalent. This cuts the number of unique combinations in half, resulting in a final count of (2ᵏ−2)/2 iterations. For our above toy example with k=5 Cyrillic letters, that number is 15. But when k=20, it balloons to 524,287 combinations — enough to significantly slow down DT training.\nWhat if one could reduce the search space from (2ᵏ−2)/2 to something more manageable — without losing the optimal split? It turns out this is indeed possible. One can show theoretically that mean target encoding enables this reduction [3]. Specifically, if the categories are arranged in order of their MTE values, and only splits that respect this order are considered, the optimal split — according to Gini impurity for classification or mean squared error for regression — will be among them. There are exactly k-1 such splits, a dramatic reduction compared to (2ᵏ−2)/2. The pseudocode for MTE is below. \nI’m not going to repeat the theoretical derivations that support the above claims. Instead, I designed an experiment to validate them empirically and to get a sense of the efficiency gains brought by MTE over native partitioning, which exhaustively iterates over all possible splits. In what follows, I explain the data generation process and the experiment setup.\nTo generate synthetic data for the experiment, I used a simple function that constructs a two-column dataset. The first column contains n distinct categorical levels, each repeated m times, resulting in a total of n × m rows. The second column represents the target variable and can be either binary or continuous, depending on the input parameter. Below is the pseudocode for this function.\nThe experiment function takes a list of cardinalities and a splitting criterion—either Gini impurity or mean squared error, depending on the target type. For each categorical feature cardinality in the list, it generates 100 datasets and compares two strategies: exhaustive evaluation of all possible category splits and the restricted, MTE-informed ordering. It measures the runtime of each method and checks whether both approaches produce the same optimal split score. The function returns the number of matching cases along with average runtimes. The pseudocode is given below.\nYou can take my word for it — or repeat the experiment (GitHub) — but the optimal split scores from both approaches always matched, just as the theory predicts. The figure below shows the time required to evaluate splits as a function of the number of categories; the vertical axis is on a logarithmic scale. The line representing exhaustive evaluation appears linear in these coordinates, meaning the runtime grows exponentially with the number of categories — confirming the theoretical complexity discussed earlier. Already at 12 categories (on a dataset with 1,200 rows), checking all possible splits takes about one second — three orders of magnitude slower than the MTE-based approach, which yields the same optimal split.\nDecision trees can natively handle categorical data, but this ability comes at a computational cost when category counts grow. Mean target encoding offers a principled shortcut — drastically reducing the number of candidate splits without compromising the result. Our experiment confirms the theory: MTE-based ordering finds the same optimal split, but exponentially faster.\nAt the time of writing, scikit-learn does not support categorical features directly. So what do you think — if you preprocess the data using MTE, will the resulting decision tree match one built by a learner that handles categorical features natively?\n[1] A Benchmark and Taxonomy of Categorical Encoders. Towards Data Science. https://towardsdatascience.com/a-benchmark-and-taxonomy-of-categorical-encoders-9b7a0dc47a8c/\n[2] Mining Rules from Data. Towards Data Science. https://towardsdatascience.com/mining-rules-from-data\n[3] Hastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2. New York: Springer, 2009.\nWritten By\nTopics:\nShare this article:\nStep-by-step code guide to building a Convolutional Neural Network \nHere’s how to use Autoencoders to detect signals with anomalies in a few lines of… \nAn illustrated guide on essential machine learning concepts \nDerivation and practical examples of this powerful concept \nColumns on TDS are carefully curated collections of posts on a particular idea or category… \nWith demos, our new solution, and a video \nAn illustrated guide to everything you need to know about Logistic Regression \nYour home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\nThese cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nUser Authorisation in Streamlit With OIDC and Google | Towards Data Science\nExploring the Proportional Odds Model for Ordinal Logistic Regression | Towards Data Science\nApplications of Density Estimation to Legal Theory | Towards Data Science\nMastering SQL Window Functions | Towards Data Science",
  "url": "https://towardsdatascience.com/decision-trees-natively-handle-categorical-data/"
}